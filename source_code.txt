# final_project.py
# does all of the work

import csv
import file_system
import clean
import naive_bayes as bayes
import table_utils
import hw4_util as homework
import constants
import analysis
import diagram
from constants import INDICES, INCOME
import partition
from partition import holdout
import output_util as output
import classifier_util
import hw4_util
import knn
import util
import decision_tree
from decision_tree import classify
from random_forest import run_a_table
from util import these, flipKeyValues
from functools import partial
from classifier_util import accuracy
import numpy

accuracy_values = []

"""
 Function that discretizes the table to make it usable for the
 classification functions and classifiers
 Parameter: income dataset
 Return: nothing
"""
def map_columns_table(table):
    table = table_utils.mapCol(table, constants.INDICES['job-type'],
                               homework.get_job_type)
    table = table_utils.mapCol(table, constants.INDICES['degree'],
                               homework.get_degree)
    table = table_utils.mapCol(table, constants.INDICES['marital-status'],
                               homework.get_marital_status)
    table = table_utils.mapCol(table, constants.INDICES['ethnicity'],
                               homework.get_ethnicity)
    table = table_utils.mapCol(table, constants.INDICES['gender'],
                               homework.get_gender)
    table = table_utils.mapCol(table, constants.INDICES['country'],
                               homework.get_country)
    table = table_utils.mapCol(table, constants.INDICES['salary'],
                               homework.get_salary)

"""
 Helper function to print the confusion matrix
 Parameters: labels: the labels for the conusion matrix
             name: the name to be displayed for the labels
 Return: a formatted confusion matrix
"""
def _printConfusionMatrix(labels, name):
    """ Prints a confusion matrix for given labels """
    output.printHeader('Confusion Matrix')
    hw4_util.print_confusion_matrix(labels, name)

"""
 Function that does all of the data visualization. In the case of this
 project, freqency diagrams and pie charts are used to displat how
 many times and how often a particular column appears in the table.
 Parameters: None
 Return: matplotlib graphs of selected columns of the dataset.
"""
def data_vis():
    table = file_system.loadTable('incomeFullNoNA.csv')

    col = util.getCol(table, INDICES['degree'])
    freqDict = analysis.frequency(col)
    diagram.pie(freqDict, 'Degree', 'Pie-Degree')

    col = util.getCol(table, INDICES['ethnicity'])
    freqDict = analysis.frequency(col)
    diagram.pie(freqDict, 'Ethnicity', 'Pie-Ethnicity')

    col = util.getCol(table, INDICES['marital-status'])
    freqDict = analysis.frequency(col)
    diagram.pie(freqDict, 'Marital Status', 'Marital-Status')

    col = util.getCol(table, INDICES['gender'])
    freqDict = analysis.frequency(col)
    diagram.pie(freqDict, 'Gender', 'Gender')

    col = util.getCol(table, INDICES['age'])
    freqDict = analysis.frequency(col)
    diagram.dot(freqDict, 'Age', 'Dot-Age')

    table = table_utils.mapCol(table, constants.INDICES['degree'],
                               homework.get_degree)
    table = table_utils.mapCol(table, constants.INDICES['marital-status'],
                               homework.get_marital_status)
    table = table_utils.mapCol(table, constants.INDICES['ethnicity'],
                               homework.get_ethnicity)
    table = table_utils.mapCol(table, constants.INDICES['salary'],
                               homework.get_salary_continuous)
    table = table_utils.mapCol(table, constants.INDICES['gender'],
                               homework.get_gender)

    col = util.getCol(table, INDICES['degree'])
    freqDict = analysis.frequency(col)
    diagram.frequency(freqDict, 'Degree', 'Frequency-Degree')

    col = util.getCol(table, INDICES['marital-status'])
    freqDict = analysis.frequency(col)
    diagram.frequency(freqDict, 'Marital Status', 'Frequency-Marital-Status')

"""
 Function that does all of the calculations, accuracies, and confusion
 matrices for both KNN and Naive Bayes.
 Parameters: income dataset
 Return: accuracies and confusion matrices for KNN and Naive Bayses classifiers.
"""
def knn_and_naive(table):
    """ Analyzes the table based on Knn and Naive Bayes
    :param table: the table of the titanic dataset
    :return: nothing
    """
    map_columns_table(table)
    table = knn.normalize_table(table, [5,7])

    # KNN
    output.printHeader('K-Nearest Neighbors')

    labels = hw4_util.random_subsample_knn(table, 5, 10, constants.INDICES['salary'])
    accuracy = classifier_util.accuracy(labels)
    print('\tRandom Subsample')
    print('\t\tAccuracy = ' + str(accuracy) + ', error rate = ' + str(1 - accuracy))

    labels = hw4_util.stratified_cross_fold_knn(table, 5, 10, constants.INDICES['salary'])

    accuracy = classifier_util.accuracy(labels)
    accuracy_values.append(accuracy)
    print('\tStratified Cross Folds (5)')
    print('\t\tAccuracy = ' + str(accuracy) + ', error rate = ' + str(1 - accuracy))

    _printConfusionMatrix(labels, 'Salary')

    # Naive Bayes
    output.printHeader('Naive Bayes')
    test_by_names = ['degree', 'ethnicity', 'gender']

    accuracy = classifier_util.accuracy(
        hw4_util.random_subsample_naive_bayes(table, 10, constants.INDICES['salary'],
                                              test_by_names))

    print('\tRandom Subsample')
    print('\t\tAccuracy = ' + str(accuracy) + ', error rate = ' + str(1 - accuracy))

    labels = hw4_util.stratified_cross_fold_naive_bayes(table, 10, constants.INDICES['salary'],
                                                        test_by_names)
    accuracy = classifier_util.accuracy(labels)
    accuracy_values.append(accuracy)
    print('\tStratified CrossFolding')
    print('\t\tAccuracy = ' + str(accuracy) + ', error rate = ' + str(1 - accuracy))
    _printConfusionMatrix(labels, 'Salary')

"""
 Function that does all of the decision tree calculations and confusion matrix
 for Decision Trees
 Parameters: income dataset
 Return: accuracies and confusion matrices for the decision tree classifier.
"""
def decisiontree(table):
    map_columns_table(table)

    output.printHeader('Decision Tree')

    attributes = these(INDICES, 'ethnicity', 'degree', 'gender')
    domains = table_utils.get_domains(table, attributes)
    tree = decision_tree.tdidt(table, attributes, domains, INDICES['salary'])

    decision_tree.print_rules(tree, ['age', 'job-type', 'degree', 'marital-status',
                    'ethnicity', 'gender', 'country', 'salary'],
                              'salary')

    attributes = these(INDICES, 'degree', 'ethnicity', 'gender')

    # Creates a myClassifier function that's paritally filled out
    # From decision_tree.classify
    # Essentially a new function:
    # myClassifier(training, test, class_index)
    myClassifier = partial(decision_tree.classify, att_indexes=attributes, att_domains=domains)

    labels = homework.stratified_cross_fold(table, 10, INDICES['salary'],
       myClassifier)

    acc = accuracy(labels)
    accuracy_values.append(acc)
    print('\n')
    print('Stratified CrossFolding')
    print('\tAccuracy = ' + str(acc) + ', error rate = ' + str(1 - acc))
    print('\n')

    # Confusion Matrix
    _printConfusionMatrix(labels, 'Salary')

"""
 Function that does all of the decision tree calculations and confusion matrix
 for Random Forests
 Parameters: income dataset
             number of trees to be generated
             number of trees to uses
             number of elements in random subsets
 Return: accuracies and confusion matrices for the random forests classifier.
"""
def randomforest(table, n, m, f):
    output.printHeader('Random Forest')
    print("N = " + str(n) + " M = " + str(m) + " F = " + str(f))
    indexes = [INDICES['degree'], INDICES['ethnicity'], INDICES['gender']]
    domains = table_utils.get_domains(table, indexes)
    forest_labels, train, test = \
                run_a_table(table, indexes,
                    INDICES['salary'], n, m, f)
    forest_accurcay = accuracy(forest_labels)

    print('\tAccuracy = ' + str(forest_accurcay))
    _printConfusionMatrix(forest_labels, 'Salary')


def main():
    # Data preprocessing
    newTable = file_system.loadTable("incomeFull.csv")
    removedRowsTable = clean.removeNA(newTable)
    incomeDataFullNoNA = file_system.write(removedRowsTable, "incomeFullNoNA.csv")
    output.printHeader('Rows with NAs have been removed.')

    # Data visualization
    data_vis()
    output.printHeader('Data visualization complete.')

    # KNN and Naive Bayes classifiers
    table = file_system.loadTable('incomeFullNoNA.csv')
    knn_and_naive(table)

    # Decision Tree classifier
    table = file_system.loadTable('incomeFullNoNA.csv')
    decisiontree(table)

    #Random Forest classifier
    table = file_system.loadTable('incomeFullNoNA.csv')
    randomforest(table, 3000, 215, 2)

if __name__ == '__main__':
main()

# analysis.py
# used for data visualization

import util
import collections

# Returns an Ordered Dictionary of frequency
def frequency(col):
    data = collections.OrderedDict()
    col = sorted(col)

    for el in col:
        if util.getFromDict(data, el) == None:
            data[el] = 1
        else:
            data[el] += 1 #Add to the count
    return data

# Sets cutoffs for the frequencies
def frequencies_for_cutoffs(col, cutoffs):
    # Sets each frequency to 0 by default
    freqs = [0] * len(cutoffs)

    for el in col:
        for i in range(len(cutoffs)):
            if el <= cutoffs[i]:
                freqs[i] += 1
                break
    return freqs

def points(table, x_index, y_index):
    vals = []
    for row in table:
        x = row[x_index]
        y = row[y_index]
        if x == 'NA' or y == 'NA':
            continue
        vals.append([x,y])
return vals

# classifier_util.py
# used for classification approaches

import util
import numpy
import table_utils

def _split(labels):
    """ Divides up the labels by their actual label
    returns a dictionary of labels
    """
    groups = {}

    for label in labels:
        actual, predicted = label

        try:
            groups[actual].append((actual, predicted))
        except KeyError:
            groups[actual] = [(actual, predicted)]

    return groups

def accuracy(labels):
    """
    Returns multiclass accuracy
    :param labels: list of tuples with [(actual, predicted), ...]
    :return: accuracy
    """
    correct = 0
    all_labels = list(set([labels[i][0] for i in range(len(labels))]))  # set of all the possible labels
    accuracies = [0] * len(all_labels)  # accuracies of each label

    for i in range(len(accuracies)):
        current_label = all_labels[i]
        for label in labels:
            actual, predicted = label
            if actual == current_label and predicted == current_label:
                accuracies[i] += 1  # true positives
            elif actual != current_label and predicted != current_label:
                accuracies[i] += 1  # true negatives

    length = len(labels)
    # Does the mean of all the (TP + TN) / (All labels)
    return numpy.mean([float(an_accuracy) / float(length) for an_accuracy in accuracies])


def _findFalses(splitLabels, forLabel):
    """ Takes an array of labels, counts the number of FP and FN """

    falses = 0
    for row in splitLabels:
        for actual, predicted in row:

            # If either the predicted or actual is the label we care about
            if actual == forLabel:
                if actual != predicted: #if label is false
                    falses += 1
            elif predicted == forLabel:
                if actual != predicted:
                    falses += 1
return falses

# clean.py
# used to clean the dataset

# Removes any rows that have a missing value
def removeNA(table):
    newTable = []
    for row in table:
        if not hasMissing(row):
            newTable.append(row)
    return newTable

# Finds out if the row has a missing value
def hasMissing(array):
    for el in array:
        if (el == 'NA'):
            return True
    return False

# Removes the spces after the comments
def removeSpaces(table):
    newTable = []
    with open(table, 'wb') as csvfile:
        writer = csv.writer(csvfile , skipinitialspace = False, delimiter=',')
        for row in table:
            writer.writerow(row)
return newTable

# constants.py
# used to get the indexes of the columns in the dataset

INDICES = {
    'age'           : 0,  # income dataset
    'job-type'      : 1,
    'degree'        : 2,
    'marital-status': 3,
    'ethnicity'     : 4,
    'gender'        : 5,
    'country'       : 6,
    'salary'        : 7

}

INCOME = {
    'age'           : 0,  # income dataset
    'job-type'      : 1,
    'degree'        : 2,
    'marital-status': 3,
    'ethnicity'     : 4,
    'gender'        : 5,
    'country'       : 6,
    'salary'        : 7
}

INCOME_HEADERS = ['age', 'job-type', 'degree', 'marital-status',
        'ethnicity', 'gender', 'country', 'salary']


# DataOperations
# Used in data visualization        
import util
import numpy
import collections
import constants

def getDeptEnergyRating(x):
    # Gets the range values for the DOE rankings
    keys     = util.getValues(constants.DOE_RATINGS)

    # Gets the left-end of the range x belongs in
    lowRange = util.getLowRange(keys, x)

    # Flips the dictionary, so we can query by value
    byValue  = util.flipKeyValues(constants.DOE_RATINGS)

    return byValue[lowRange]


# Approach 1
def getFreqDictByDOE(col):
    # Create empty dict
    keys = range(1, 11)
    freq = util.createDict(keys, [0] * len(keys))

    for el in col:
        rating = getDeptEnergyRating(el)
        freq[rating] += 1
    return freq

# Approach 2
def getFreqByEqualWidths(col, num_bins):
    keys = sorted(util.getBins(col, num_bins))
    dictionary = util.createDict(keys, [0] * len(keys))
    freq = collections.OrderedDict(sorted(dictionary.items()))

    for el in col:
        key = util.getLowRange(keys, el)
        freq[key] += 1
    return freq

def getAllOfOrigin(originDict, origin):
   return [value[origin] for key, value in originDict.iteritems()]

# decision_tree.py
# used to do all the decision tree calculations

import table_utils
from util import these, appendToDict
import math
from random import shuffle
# Key words
ATTRIBUTE = 'Attribute'
VALUE = 'Value'
LEAVES = 'Leaves'

def classify(training, test, class_index, att_indexes, att_domains):
    indexes = att_indexes[:]
    tree = tdidt(training, indexes, att_domains, class_index)

    return [(row[class_index], get_label(tree, row)) for row in test]


def get_label(decision_tree, instance):
    """  predicts the label of the given instance from the decision tree
    :param decision_tree: the decision tree to make the prediction from
    :param instance: a row to predict the class_label of
    :return: the predicted label for the instance
    """
    if decision_tree[0] == 'Leaves':
        # get the predicted class value
        return _majority_voting(decision_tree)
    elif decision_tree[0] == 'Attribute':
        # otherwise we check an attribute
        instance_value = instance[decision_tree[1]]
        for i in range(2, len(decision_tree)):
            if instance_value == decision_tree[i][1]:
                return get_label(decision_tree[i][2], instance)
    else:
        # something is wrong with the tree
        print 'Failed label on ' + instance
        return -1


def _majority_voting(leaves):
    """ Given leaf nodes it will return the most probable class_label
    :param leaves: leaf nodes
    :return: class label
    """
    highest_prob = None
    highest_class_value = None
    for i in range(1, len(leaves)):
        if (highest_prob == None or
                    highest_prob < leaves[i][3]):
            highest_prob = leaves[i][3]
            highest_class_value = leaves[i][0]
    return highest_class_value


def print_rules(tree, names, class_name='class'):
    """ Prints the rules of the tree in the format of
    IF att == val AND ... THEN class = label
    IF att == val AND ... THEN class = label
    :param tree: a decision tree in the format of the tdidt function
    :param names: the names of the indexes
    :param class_name: the names of the class_label
    """
    if tree[0] == 'Leaves':
        _print_rules_helper(tree, '', names, class_name)
    else:
        current_name = names[tree[1]]
        rule = 'IF ' + str(current_name) + ' == '
        for i in range(2, len(tree)):
            new_rule = rule
            current_value = tree[i][1]
            new_rule += str(current_value) + ' '
            _print_rules_helper(tree[i][2], new_rule, names, class_name)


def _print_rules_helper(tree, old_rule, names, class_name):
    """ Prints the rules of the tree in the format of
    IF att == val AND ... THEN class = label
    IF att == val AND ... THEN class = label
    :param tree: a decision tree in the format of the tdidt function
    :param old_rule: the current base rule that the printer is on
    :param names: the names of the indexes
    :param class_name: the names of the class_label
    """
    if tree[0] == 'Leaves':
        print old_rule + 'THEN ' + class_name + ' = ' + _majority_voting(tree)
    else:
        current_name = names[tree[1]]
        rule = 'AND ' + str(current_name) + ' == '
        for i in range(2, len(tree)):
            new_rule = old_rule
            new_rule += rule
            current_value = tree[i][1]
            new_rule += str(current_value) + ' '
            _print_rules_helper(tree[i][2], new_rule, names, class_name)


def tdidt(table, att_indexes, att_domains, class_index):
    """ creates a decision tree for the table
    tree format:
    [`Attribute', `0',
        [`Value', `1', [`Leaves', [`yes',20,20,100%]]]
        [`Value', `2',
            [`Attribute', `2',
                [`Value', `fair',
                    [`Leaves', [`yes',4,10,40%], [`no',6,10,60%]]]
            ]
        ]
    ]
    :param table: a table (assuming it is not empty
    :param att_indexes: the attributes to be partitioned on
    :param att_domains: the values of the corresponding att_domains (in a dict)
    :param class_index: the index of the class_label
    :return: the decision tree
    """
    # First check for if there should be a leaf node
    stats = partition_stats(table, class_index)
    # check to see if there is only one class label present
    # or there are still attributes to partition on
    if same_class(table, class_index) or att_indexes == []:
        return create_leaf_node(stats)

    # check to make sure all partitions have instances
    index = select_best_partition_index(table, att_indexes, class_index)
    partitions = partition_instances(table, index)
    have_instances = True

    if len(att_domains[index]) > len(partitions):
        # this means that there wasn't a partition for one of the attribute values
        return create_leaf_node(stats)

    # Partition Recursion Time!
    att_indexes.remove(index)

    tree = ['Attribute', index]
    for att_value, partition in partitions.items():
        # For each partition will do recursive creation
        subtree = ['Value', att_value]
        subtree.append(tdidt(partition, att_indexes, att_domains,class_index))
        tree.append(subtree)
    return tree

def classify_with_tree(tree, class_index, test):
    return [(row[class_index], get_label(tree, row)) for row in test]

def random_attribute_subset(indexes, F):
    # shuffle and pick first F
    shuffled = indexes[:]  # make a copy
    shuffle(shuffled)
    return shuffled[:F]

def tdidt_RF(table, att_indexes, att_domains, class_index, F=0):
    """ creates a decision tree for the table
    tree format:
    [`Attribute', `0',
        [`Value', `1', [`Leaves', [`yes',20,20,100%]]]
        [`Value', `2',
            [`Attribute', `2',
                [`Value', `fair',
                    [`Leaves', [`yes',4,10,40%], [`no',6,10,60%]]]
            ]
        ]
    ]
    :param table: a table (assuming it is not empty
    :param att_indexes: the attributes to be partitioned on
    :param att_domains: the values of the corresponding att_domains (in a dict)
    :param class_index: the index of the class_label
    :return: the decision tree
    """
    # First check for if there should be a leaf node
    stats = partition_stats(table, class_index)
    # check to see if there is only one class label present
    # or there are still attributes to partition on
    if same_class(table, class_index) or att_indexes == []:
        return create_leaf_node(stats)

    indexes_to_use = att_indexes

    # uses random subset of indexes if doing random forest
    if 0 < F < len(att_indexes):
        indexes_to_use = random_attribute_subset(att_indexes, F)

    index = select_best_partition_index(table, indexes_to_use, class_index)
    partitions = partition_instances(table, index)
    have_instances = True

    # check to make sure all partitions have instances
    if len(att_domains[index]) > len(partitions):
        # this means that there wasn't a partition for one of the attribute values
        return create_leaf_node(stats)

    # Partition Recursion Time!
    att_indexes.remove(index)

    tree = ['Attribute', index]
    for att_value, partition in partitions.items():
        # For each partition will do recursive creation
        subtree = ['Value', att_value]
        subtree.append(tdidt_RF(partition, att_indexes,
                             att_domains,class_index, F))
        tree.append(subtree)
    return tree


def create_leaf_node(stats):
    """ Creates a leaf node
    :param stats: stats on the partition
    :return: a leaf node in the form ['Leaves', ['yes', 4, 10, 40%],...]
    """
    tree = ['Leaves']
    for row in stats:
        if (row[1] > 0):
            tree.append(row)
    return tree


def same_class(instances, class_index):
    """ Determines if all the instances have the same class label
    :param instances: table
    :param class_index: index of the class label
    :return: True if all have same class label flase otherwise
    """
    label = instances[0][class_index]
    for row in instances:
        if row[class_index] != label:
            return False
    return True


def partition_stats(instances, class_index):
    """ Returns the stas of the class_labels in partition
    :param instances: table
    :param class_index: index of class_label
    :return: list of stats ex:[['yes', 4, 10], ...]
    """

    # Count instances of class label
    counts = {}
    for row in instances:
        class_value = row[class_index]
        try:
            counts[class_value] += 1
        except KeyError:
            counts[class_value] = 1

    # Create stats from count dictionary
    stats = []
    total = len(instances)
    for label, count in counts.iteritems():
        stats.append([str(label), count, total, count / (total * 1.0)])

    return stats


def partition_instances(table, att_index):
    """ partitions the table based on an attribute
    :param table: a table
    :param att_index: the index of attribute to be partitioned on
    :return: dictionary {att_val1: part1, att_val2: part2, ...}
    """
    partitions = {}
    for row in table:
        att_value = row[att_index]
        partitions = appendToDict(partitions, att_value, row)

    return partitions


def select_best_partition_index(instances, att_indexes, class_index):
    """ determines the best attribute to partition on
    :param instances: a table
    :param att_indexes: the indexes of the attributes
    :param class_index: the index of the class_label
    :return: the index of the attribute to partition on
    """
    best_index = None
    lowest_e_new = None

    for index in att_indexes:
        e_new = calc_e_new(instances, index, class_index)
        if lowest_e_new == None or lowest_e_new > e_new:
            lowest_e_new = e_new
            best_index = index

    return best_index


def calc_e_new(instances, att_index, class_index):
    """ calculates Enew for what partitioning on the instnace would give
    :param instances: a table
    :param att_index: index to calculate partition on
    :param class_index: index of the class label
    :return: Enew
    """
    # get the length of the partition
    D = len(instances)
    # calculate the partition stats for att_index (see below)
    freqs = att_freqs(instances, att_index, class_index)
    # find E_new from freqs (calc weighted avg)
    E_new = 0
    for att_val in freqs:
        D_j = float(freqs[att_val][1])
        probs = [(c / D_j) for (_, c) in freqs[att_val][0].items()]
        # changed this to handle when p < 0 because log(anything less than 0)
        # is undefined and will throw an error.
        E_D_j = -sum([p * math.log(p, 2) for p in probs if p > 0])
        E_new += (D_j / D) * E_D_j
    return E_new


def att_freqs(instances, att_index, class_index):
    """ gives the stats the distribution of class_labels given an index
    :param instances: a table
    :param att_index: the index of the attribute to get class_label stats on
    :param class_index: the index of the class_labels
    :return: {att_val:[{class1: freq, class2: freq, ...}, total], ...}
    """
    # get unique list of attribute and class values
    att_vals = list(set(table_utils.getCol(instances, att_index)))
    class_vals = list(set(table_utils.getCol(instances, class_index)))
    # initialize the result
    result = {v: [{c: 0 for c in class_vals}, 0] for v in att_vals}
    # build up the frequencies
    for row in instances:
        label = row[class_index]
        att_val = row[att_index]
        result[att_val][0][label] += 1
        result[att_val][1] += 1
return result

# diagram.py
# used for plotting graphs for data visuealization

import matplotlib
matplotlib.use('pdf')
import matplotlib.pyplot as pyplot
import numpy
import util
import analysis
import math_utils
import dataOperations

figureFolder = 'graphs/'


def frequency(frequencyDict, name, title):
    pyplot.figure()

    # Create values
    ys = util.getValues(frequencyDict)
    xs = util.getKeys(frequencyDict)
    xrng = numpy.arange(len(xs))

    # Create plot
    pyplot.suptitle('Frequency of ' + name)
    pyplot.bar(xrng, ys, alpha=0.75, align='center')
    pyplot.xticks(xrng, xs)

    # Save plot
    filename = title + '.pdf'
    pyplot.savefig(figureFolder + filename)
    pyplot.xlabel(name)
    pyplot.ylabel('Count')

    pyplot.figure() # Reset for good measure
    pyplot.close()

def frequencyWithRanges(frequencyDict, name, xlabels, title):
    pyplot.figure()

    # Create values
    ys = util.getValues(frequencyDict)
    xs = util.getKeys(frequencyDict)
    xrng = numpy.arange(len(xs))

    # Create plot
    pyplot.suptitle(name)
    pyplot.bar(xrng, ys, alpha=0.75, align='center')
    pyplot.xticks(xrng, xlabels)
    pyplot.ylabel('Count')

    # Save plot
    filename = title + '.pdf'
    pyplot.savefig(figureFolder + filename)

    pyplot.figure() # Reset for good measure
    pyplot.close()

def pie(frequencyDict, name, step):
    pyplot.figure(figsize=(20,20))

    # Create values
    ys = util.getValues(frequencyDict)
    xs = util.getKeys(frequencyDict)

    # Create plot
    pyplot.suptitle(name)
    pyplot.pie(ys, labels=xs, autopct='%1.1f%%')
    labels = [r'High School', r'Bachelors', r'masters', r'doctorate', r'dropout'
        , r'Assiciate', r'Middleschool', r'Elementary', r'Prof-school', r'other']
    pyplot.legend(labels)
    pyplot.legend(loc=2, prop={'size':15})
    pyplot.tight_layout()

    # Save plot
    filename = str(step) + '-pie-' + name + '.pdf'
    pyplot.savefig(figureFolder + filename)

    pyplot.figure() # Reset for good measure
    pyplot.close()

def dot(frequencyDict, name, step):
    pyplot.figure()

    # Create values
    xs = util.getKeys(frequencyDict)
    xs.sort() # Sort the x values
    ys = [1] * len(xs)

    # Create Plot
    pyplot.suptitle(name)
    pyplot.plot(xs, ys, 'b.', alpha=0.2, markersize=6)
    pyplot.gca().get_yaxis().set_visible(False)
    pyplot.xlabel(name)

     # Save plot
    filename = str(step) + '-dot-' + name + '.pdf'
    pyplot.savefig(figureFolder + filename)

    pyplot.figure() # Reset for good measure

def hist(frequencyDict, name, step):
    pyplot.figure()

    # Create values
    xs = util.getKeys(frequencyDict)
    max_val = max(xs)
    min_val = min(xs)
    increment = int((max_val - min_val)) / 10
    xticks = numpy.arange(min_val, max_val, float(increment))

    # Create plot
    pyplot.suptitle(name)
    pyplot.hist(xs, bins=10, alpha=0.95, color='r', align='left', rwidth=1)
    pyplot.xticks(xticks)

    # Save plot
    filename = str(step) + '-hist-' + name + '.pdf'
    pyplot.savefig(figureFolder + filename)
    pyplot.xlabel(name)
    pyplot.ylabel('Count')

    pyplot.figure() # Reset for good measure

def scatter(points, name, step):
    pyplot.figure()

    # Create values
    xs = util.getCol(points, 0)
    ys = util.getCol(points, 1)

    # Create plot
    pyplot.suptitle(name + ' vs. ' + 'MPG')
    pyplot.plot(xs, ys, 'b.')
    pyplot.xlim(0, int(max(xs) * 1.1))
    pyplot.ylim(0, int(max(ys) * 1.1))
    pyplot.grid(True)
    pyplot.ylabel('MPG')
    pyplot.xlabel(name)

    # Save plot
    filename = str(step) + '-scatter-' + name + '.pdf'
    pyplot.savefig(figureFolder + filename)
    pyplot.figure() # Reset for good measure
    pyplot.close()

def box(arrays, name, xlabels, step):
    pyplot.figure(figsize=(50,50))
    fig, ax = pyplot.subplots()

    ax.boxplot(arrays)
    pyplot.xticks(numpy.arange(1, len(xlabels)+10, 1), xlabels)

    filename = str(step) + '-box-' + name + '.pdf'
    pyplot.savefig(figureFolder + filename)
    pyplot.close()

def countryListFreq(countriesDict):
    pyplot.figure()

    europes = dataOperations.getAllOfOrigin(countriesDict, 'Europe')
    japans = dataOperations.getAllOfOrigin(countriesDict, 'Japan')
    americas = dataOperations.getAllOfOrigin(countriesDict, 'USA')
    years = util.getKeys(countriesDict)

    fig, ax = pyplot.subplots()

    ind = numpy.arange(len(years))
    width = 0.2

    r1 = ax.bar(ind, europes, width, color='b')
    r2 = ax.bar(ind+width, japans, width, color='r')
    r3 = ax.bar(ind+(width*2), americas, width, color='g')

    ax.set_xticks(ind + (width*2))
    ax.set_xticklabels(years)
    ax.set_ylabel('Number of Cars')
    ax.set_xlabel('Year')

    ax.legend((r1[0], r2[0], r3[0]), ('Europe', 'Japan', 'USA'), loc=1)

    pyplot.savefig(figureFolder + 'step-8-country-list.pdf')
    pyplot.close()

def scatterWithLine(points, name, step, ylabel='MPG', xlabel=None, usesMPG=True):
    pyplot.figure()

    xs = util.getCol(points, 0)
    ys = util.getCol(points, 1)

    correlationCoeff = str(round(math_utils.correlationCoeff(points), 2))
    covariance = str(round(math_utils.covariance(points), 2))

    vs = ''
    if (usesMPG):
        vs = 'vs. MPG; '

    firstPart = name + vs + ' Correlation: '
    title = firstPart + correlationCoeff + ' Covariance: ' + covariance

    # Create plot
    pyplot.suptitle(title)
    pyplot.plot(xs, ys, 'b.')
    pyplot.xlim(0, int(max(xs) * 1.1))
    pyplot.ylim(0, int(max(ys) * 1.1))
    pyplot.grid(True)
    pyplot.ylabel(ylabel)
    if not xlabel:
        pyplot.xlabel(name)
    else:
        pyplot.xlabel(xlabel)

    linePoints = math_utils.graphablePoints(points)
    lineXs = util.getCol(linePoints, 0)
    lineYs = util.getCol(linePoints, 1)
    pyplot.plot(lineXs, lineYs, 'r')

    # Save plot
    filename = str(step) + '-scatter-regression-' + name + '.pdf'
    pyplot.savefig(figureFolder + filename)

    pyplot.figure()
pyplot.close()


# filesystem.py
# used for loading and writing to a table

import csv
import util

# Load a table from a file with no fuss
def loadTable(filename):
    the_file = open(filename, 'r')
    the_reader = csv.reader(the_file, dialect='excel')
    table = []
    for row in the_reader:
        if len(row) > 0:
            table.append(util.listToCorrectType(row))
    the_file.close()
    return table

def write(table, filename):
    with open(filename, 'wb') as csvfile:
        writer = csv.writer(csvfile , skipinitialspace=False, delimiter=',')
        for row in table:
writer.writerow(row)

# project_util.py
# does the calculations for the algorithims

from tabulate import tabulate
import homework_util as homework
import naive_bayes as bayes
import util
import table_utils
import partition
import knn

def random_subsample_naive_bayes(table, k, class_index, test_by_names, contIndices=[]):
    """ Uses Naive Bayes and Random Subsampling to find labels """
    return _random_subsample(table, k, predict_labels, class_index, test_by_names, contIndices)


def random_subsample_knn(table, num_holdout, k, class_index):
    """ Uses knn and random subsampling to predict labels
    :param table: a table of data
    :param num_holdout: the number of random subsamples to run
    :param k: the number of nearest nieghbors
    :param class_index: the index where the class label is
    :return: eturns a list of tuples of the labels in the form [(actual, predicted),...]
    """
    labels = []

    # repeats the subsampling k times
    for i in range(num_holdout):
        test_set, training_set = partition.holdout(table)

        # gets the lables
        labels.extend(knn.knn(training_set, test_set, k, class_index))
    return labels

def stratified_cross_fold_knn(table, num_folds, k, class_index):
    """ Uses knn and stratified cross folding to predict labels
    :param table: a table of data
    :param num_folds: the number of folds to calculate
    :param k: the number of nearest nieghbors
    :param class_index: the index where the class label is
    :return: returns a list of tuples of the labels in the form [(actual, predicted),...]
    """
    labels = []
    folds = strat_folds(table, class_index, num_folds)
    for index in range(len(folds)):
        test = folds[index]  # creates the test from one fold
        training = []

        # meshes the rest of the folds together to make the training set
        for training_index in range(len(folds)):
            if training_index != index:
                training.extend(folds[training_index])

        labels.extend(knn.knn(training, test, k, class_index))
    return labels

def _random_subsample(table, k, classify, class_index, test_by_names, contIndices=[]):
    """
    Uses random subsampling to test labels on classify
    :param table: Table of data
    :param k: Number of times to run holdout method
    :return: labels in format list of tuples [(actual, predicted),...]
    """
    labels = []

    # repeats the subsampling k times
    for i in range(k):
        test_set, training_set = partition.holdout(table)

        # gets the lables
        labels.extend(classify(training_set, test_set, class_index, test_by_names, contIndices))
    return labels


def predict_labels(training, test, class_index, test_by_names, contIndices=[]):
    """ Using Naive Bayes it will predict values in test by using training
    :param training: A table of data
    :param test: A table of data in the same format as training
    :return: labels in format list of tuples [(actual, predicted),...]
    """
    labels = []
    for instance in test:
        actual = instance[class_index]  # the actual label of the instance

        # change the instance to the values we are testing from
        instance = homework.getNamedTuples(instance, test_by_names)

        # gets the predicted label
        predicted, probability = bayes.predict_label(training,
            instance, class_index, contIndices)

        # records the two labels
        labels.append((actual, predicted))
    return labels


def stratified_cross_fold_naive_bayes(table, k, class_index, test_by_names, contIndices=[]):
    """ Runs naive bayes on k stratified cross folds """
    return _stratified_cross_fold(table, k, predict_labels, class_index, test_by_names, contIndices)


def _stratified_cross_fold(table, k, classify, class_index, test_by_names, contIndices=[]):
    """
    Uses stratified crossfolding to create the training and test sets
    :param table: Table of data
    :param k: Number of folds
    :return: labels in format list of tuples [(actual, predicted),...]
    """
    labels = []
    folds = strat_folds(table, class_index, k)
    for index in range(len(folds)):
        test = folds[index]  # creates the test from one fold
        training = []

        # meshes the rest of the folds together to make the training set
        for training_index in range(len(folds)):
            if training_index != index:
                training.extend(folds[training_index])

        labels.extend(classify(training, test, class_index, test_by_names, contIndices))
    return labels


def strat_folds(table, by_label_index, k):
    """
    Creates fold where each fold has the same distrubution of class labels as the origonal table
    :param table: table of data
    :param by_label_index: the class label index
    :param k: the number of partitions to create
    :return: a list of tables where each table is a folds, i.e.: [[P1],[P2],..., [Pnum]] where each P is a table
    """
    labels_to_rows = {}

    # spreads the data out into a dictionary where the key is the class label and the data is a table consisting of
    # rows with that class label {class_label:rows_with_class_label
    for row in table:
        label = row[by_label_index]
        try:
            labels_to_rows[label].append(row)
        except KeyError:
            labels_to_rows[label] = [row]

    # creates folds by evenly distributing the rows of each class label to the number of partitions
    folds = {}
    index = 0
    for key, table in labels_to_rows.iteritems():
        for row in table:
            try:
                folds[index].append(row)
            except KeyError:
                folds[index] = [row]
            index += 1
            if index > k:
                index = 0
    return util.dictionaryToArray(folds)

def print_confusion_matrix(labels, class_label_name):
    """ Prints the confusion matrix of the given labels
    :param labels: A list of tuples of class labels [(actual, predicted),...]
    :param class_label_name: The name of the class label
    """
    class_labels = list(set(table_utils.getCol(labels, 0)))  # all the actual class labels
    the_headers = [class_label_name]
    the_headers.extend(class_labels)
    the_headers.extend(['Total', 'Recognition (%)'])

    # makes an table filled with zeros of #columns = len(the_headers) and #rows = len(class_labels)
    confusion_matrix = [[0] * len(the_headers) for i in range(len(class_labels))]

    # fills out the confusion matrix with the predicted vs. actual
    for a_label_point in labels:
        actual, predicted = a_label_point
        confusion_matrix[class_labels.index(actual)][the_headers.index(predicted)] += 1

    # add the rest of the values to the confusion matrix
    for i in range(len(confusion_matrix)):
        row = confusion_matrix[i]  # current row

        # adding total to the confusion matrix
        total = sum(row)
        row[the_headers.index('Total')] = total  # add the total in for the row

        row[0]= class_labels[i]  # adds the class label for the row to the beginning of row

        # adding recognition to the confusion matrix (% of guesses in row that are correct
        recognition = row[the_headers.index(class_labels[i])] # TP
        recognition /= float(total)
        recognition *= 100
        row[the_headers.index('Recognition (%)')] = recognition

    # prints the table
    print tabulate(confusion_matrix, headers = the_headers, tablefmt="rst")

def stratified_cross_fold(table, k, class_index, classify, *opts):
    """
    Uses stratified crossfolding to predict labels
    :param table: Table of data
    :param k: Number of folds
    :param class_index: the class's index
    :param classify: a function to classify on
    :param opts: anything else you'd like to pass into the classify
    :return: labels in format list of tuples [(actual, predicted),...]
    """

    labels = []
    folds = strat_folds(table, class_index, k)
    for index in range(len(folds)):
        test = folds[index]  # creates the test from one fold
        training = []

        # meshes the rest of the folds together to make the training set
        for training_index in range(len(folds)):
            if training_index != index:
                training.extend(folds[training_index])

        labels.extend(classify(training, test, class_index, *opts))
    return labels

def strat_folds(table, by_label_index, k):
    """
    Creates fold where each fold has the same distrubution of class labels as the origonal table
    :param table: table of data
    :param by_label_index: the class label index
    :param k: the number of partitions to create
    :return: a list of tables where each table is a folds, i.e.: [[P1],[P2],..., [Pnum]] where each P is a table
    """
    labels_to_rows = {}

    # spreads the data out into a dictionary where the key is the class label and the data is a table consisting of
    # rows with that class label {class_label:rows_with_class_label
    for row in table:
        label = row[by_label_index]
        try:
            labels_to_rows[label].append(row)
        except KeyError:
            labels_to_rows[label] = [row]

    # creates folds by evenly distributing the rows of each class label to the number of partitions
    folds = {}
    index = 0
    for key, table in labels_to_rows.iteritems():
        for row in table:
            try:
                folds[index].append(row)
            except KeyError:
                folds[index] = [row]
            index += 1
            if index > k:
                index = 0
    return util.dictionaryToArray(folds)


def convertRowIntoIndexValuePairs(row):
    """ Converts [x, y, z, ...] into [(0, x), (1, y), (2, z)]
    for use in the classifiers in their "where" statements
    """
    return [ (index, value) for index, value in enumerate(row)]


def getNamedTuples(row, names):
    """ Gets a bunch of tuples by their name
    Ex: getNamedColsFromRow(row, 'mpg', 'origin', 'weight')
        might return [(0, 18.0), (4, 3504), (7, 1)]
    WARNING: These don't necessarily return in any specific order.
    """
    tuples = []
    namesIndexes = [constants.INDICES[name] for name in names]
    for index, value in enumerate(row):
        if index in namesIndexes:
            tuples.append((index, value))
    return tuples

def get_job_type(x):
    """ returns the int value for the nominal value job-type
    """
    if x == 'Government':
        return 1
    elif x == 'Private':
        return 2
    elif x == 'Self-employed':
        return 3
    else:
        return 0

def get_degree(x):
    """ returns the int value for the nominal value degree
    """
    if x == 'HS':
        return 1
    elif x == 'Bachelors':
        return 2
    elif x == 'Masters':
        return 3
    elif x == 'Doctorate':
        return 4
    elif x == 'College-drop-out':
        return 5
    elif x == 'Associate':
        return 6
    elif x == 'Middleschool':
        return 7
    elif x == 'Elementary':
        return 8
    elif x == 'Prof-school':
        return 9
    else:
        return 0


def get_marital_status(x):
    """ returns the int value for the nominal value marital-status
    """
    if x == 'Never-married':
        return 1
    elif x == 'Married-civ-spouse':
        return 2
    elif x == 'Divorced':
        return 3
    elif x == 'Married-spouse-absent':
        return 4
    elif x == 'Widowed':
        return 5
    elif x == 'Separated':
        return 6
    elif x == 'Married-AF-spouse':
        return 7
    else:
        return 0


def get_ethnicity(x):
    """ returns the int value for the ordinal value ethnicity
    """
    if x == 'White':
        return 1
    elif x == 'Black':
        return 2
    elif x == 'Native American':
        return 3
    elif x == 'Asian':
        return 4
    else:
        return 0

def get_gender(x):
    """ returns the int value for the ordinal value gender
    """
    if x == 'Male':
        return 1
    else:
        return 0

def get_country(x):
    """ returns the int value for the ordinal value country
     """
    if x == 'United-States':
        return 1
    elif x == 'Philippines':
        return 2
    elif x == 'Puerto-Rico':
        return 3
    elif x == 'Mexico':
        return 4
    elif x == 'Dominican-Republic':
        return 5
    elif x == 'Portugal':
        return 6
    elif x == 'Canada':
        return 7
    elif x == 'Taiwan':
        return 8
    elif x == 'Cuba':
        return 9
    elif x == 'Jamaica':
        return 10
    else:
        return 0

def get_salary(x):
    """ returns the int value for the ordinal salary
    """
    if x == '>50K':
        return '1'
    else:
        return '0'

def get_salary_continuous(x):
    """ returns the int value for the ordinal salary
    """
    if x == '>50K':
        return 1
    else:
return 0

# knn.py
# does knn calculations

import table_utils
import numpy
from random import randint


def _get_top_k(row_distances, k):
    """ Gets the top k tuples with the smallest distance by
    continually trying to replace the tuple with the highest distance
    with a tuple with lower distance
    :param row_distances: a list in the form of [(dist, row),...]
    :param k: the number of top rows to pick
    :return: Returns the top k tuples with the smallest distance
    """
    smallest_tuples = []

    for tup in row_distances:
        if len(smallest_tuples) < k:
            smallest_tuples.append(tup)
            continue
        largest_index, largest = _largest(smallest_tuples)
        if tup[0] < largest[0]:
            smallest_tuples[largest_index] = tup
        elif tup[0] == largest[0] and randint(1,2) == 1:  # randomly select to tie break
            smallest_tuples[largest_index] = tup
    return smallest_tuples


def _largest(smallest_tuples):
    """ Finds the tuple with the largest distance and
    The index that it is located
    :param smallest_tuples: a list in the form of [(dist, row),...]
    :return: the index of the tuple with the highest distance and the tuple
    """
    index = None
    large_tuple = None
    for the_index in range(len(smallest_tuples)):
        if index == None or large_tuple[0] < smallest_tuples[the_index][0]:
            index = the_index
            large_tuple = smallest_tuples[the_index]
    return index, large_tuple


def _select_class_label(top_k_rows, label_index):
    """ Gets the class label based on majority voting
    :param top_k_rows: a list in the form of [(dist, row),...]
    :param label_index: the index in the rows where the label is located
    :return: the most common label
    """
    labels = []
    for tuple in top_k_rows:
        dist, row = tuple
        labels.append(row[label_index])
    return max(set(labels), key=labels.count)  #calculates mode of the instances


def _distance(row, instance, labelIndex):
    """ Calculates euclidean distance
    If the two items are catigorical, it will
    say a distance of 0 if they are the same,
    and a distance of 1 if they are different
    :param row: a row from a dataset
    :param instance: a row from the dataset to compare distance to
    :param labelIndex: the index
    :return: the distance between the row and distance
    """
    parts = []
    for i in range(len(instance)):
        # Don't include label in sum
        if i == labelIndex:
            continue

        ai = row[i]
        bi = instance[i]

        if isinstance(ai, basestring) or isinstance(bi, basestring):
            part = 0 if ai == bi else 1
            parts.append(pow(part, 2)) # raised to 2 for s's & g's
            continue
        else:
            parts.append(pow((ai - bi), 2))
            continue
    return numpy.sqrt(float(sum(parts)))


def get_label_k_nn(training_set, instance, k, label_index):
    """ Calculates a label using K_nn
    :param training_set: a table of data
    :param instance: an instance to be fitted to the training_set
    :param k: the number of nearest neighbors
    :param label_index: the index where the class label is located
    :return: the class label prediction
    """
    row_distances = []
    for row in training_set:
        d = _distance(row, instance, label_index)
        row_distances.append((d, row))
    top_k_rows = _get_top_k(row_distances, k)
    label = _select_class_label(top_k_rows, label_index)
    return label

def knn(training, test, k, class_index):
    """ Predicts the labels in test set based on knn
    :param training: a training table
    :param test: a test table
    :param k: number of nearest neighbors
    :param class_index: the index where the label is
    :return: a lis of tuples depicting the actual class label and the predicted one
    """

    labels = []
    for instance in test:
        actual = instance[class_index]
        predicted = get_label_k_nn(training, instance,  k, class_index)
        labels.append((actual,predicted))
    return labels


def normalized_value(xs):
    """ normalizes a list of numbers
    :param xs: a list of numbers
    :return: a normalized list
    """
    minval = min(xs)
    maxval = max(xs)
    minmax = (maxval-minval) * 1.0
    return [(x - minval) / minmax for x in xs]


def normalize_table(table, except_for=None):
    """ Assumes table has been cleaned of all NA values
    :param table: a data_table
    :param except_for: a list of indexes to not normalize in the table
    :return: A normalized table
    """
    new_table = [[] for i in range(len(table))]

    indexes = range(len(table[0]))  # number of indexes in a row

    for index in indexes:
        data_column = table_utils.getCol(table, index)
        if index not in except_for:
            data_column = normalized_value(data_column)  # normalize data in column

        # puts the values of the data column into the new_table
        for row_index in range(len(table)):
            new_table[row_index].append(data_column[row_index])
    return new_table

    # random_forest.py
# does the random forest calcutlaions

import decision_tree
import partition
import heapq
import table_utils
import classifier_util
from homework_util import strat_folds


def run_a_table(table, indexes, class_index, N, M, F):
    """ Takes a table, splits it into a training and test set. Creates a
    random forest for the training set. Then tests the forest off of
    the test set
    :param table: a table of values
    :param indexes: The indexes to partition on
    :param class_index: The index of the label to predict
    :param N: Number of trees to produce
    :param M: Number of the best trees to choose
    :param F: Subset size of random attributes
    :return: Returns a list of tuples. Of the actual, predicted label and
            training and test
             [(actual1,predicted1), (actual2,predicted2), ...], training, test
    """
    domains = table_utils.get_domains(table, indexes)
    folds = strat_folds(table, class_index, 3)
    training = folds[0]
    training.extend(folds[1])
    test = folds[2]
    forest = _random_forest(test, indexes, class_index, domains, N, M, F)

    return [(row[class_index], predict_label(forest, row)) for row in test], \
           training, test


def _random_forest(table, indexes, class_index, att_domains, N, M, F):
    """ Generates a random forest classifier for a given table
    :param table: a table
    :param indexes: a list of indexes to partition on
    :param class_index: the index of the class label to predict
    :param N: Number of trees to produce
    :param M: Number of the best trees to choose
    :param F: Subset size of random attributes
    :return: A list of lists. Trees and thier accuracies
            [(accuracy1, tree1), ... , (accuracyM, treeM)]
    """

    # We store the accuracies and trees in a priority queue
    # lower numbers = higher priority
    priority_queue = [] # see: https://docs.python.org/3/library/heapq.html#basic-examples
    attributes = indexes

    # Uses a training and remainder set from bootsraping to create each tree
    bags = partition.bagging(table, N)
    for bag_set in bags:
        tree = decision_tree.tdidt_RF(bag_set[0], attributes, att_domains, class_index, F)
        acc = _accuracy_for_tree(tree,class_index, bag_set[1])
        heapq.heappush(priority_queue, (acc, tree))
        #push to the priorityQueue

    # Since our priority queue is backwards (and I dunno how to reverse that)
    # we pop off all the ones we don't need. N - M
    for i in range(N - M):
        heapq.heappop(priority_queue)

    # Now our priority queue will be our list that we can return
    return priority_queue


def _accuracy_for_tree(tree, class_index, test_set):
    labels = decision_tree.classify_with_tree(tree, class_index, test_set)
    return classifier_util.accuracy(labels)


def predict_label(forest, instance):
    """ predicts the label of an instance given a forest using weighted
        voting with accuracies
    :param forest: a list of lists in te form returned by random_forest()
    :param instance: an row to have a class label predicted
    :return: a class label
    """
    labels = {}
    for acc_and_tree in forest:
        prediction = decision_tree.get_label(acc_and_tree[1], instance)
        # totals the accuracy predicted for each label
        try:
            labels[prediction] += acc_and_tree[0]
        except KeyError:
            labels[prediction] = acc_and_tree[0]

    # gets the label with the highest predicted value
    highest_value = 0
    highest_label = 0
    for current_label, value in labels.items():
        if value > highest_value:
            highest_label = current_label

return highest_label