### hw6.py

import table_utils
import file_system
import homework_util as homework
from constants import INDICES, WISCONSIN
from random_forest import run_a_table
import output_util as output
from classifier_util import accuracy
from decision_tree import classify
from homework_util import print_confusion_matrix
import util
from partition import holdout


def map_auto_cols(table):
    """ Discretizes the auto_table

    :param table: the auto_table
    :return: discretized table
    """
    mpg_index = INDICES['mpg']
    table = table_utils.mapCol(table, mpg_index, homework.getDeptEnergyRating)
    weight_index = INDICES['weight']
    table = table_utils.mapCol(table, weight_index, homework.getNHTSASize)

    return table


def step_two(auto_table, titanic_table):
    output.printHeader('Step Two: N = 20, M = 7, F = 2')
    # Titanic stuff
    titanic_indexes = [INDICES['age'], INDICES['sex'], INDICES['class']]
    titanic_domains = table_utils.get_domains(titanic_table, titanic_indexes)
    titanic_forest_labels, titanic_train, titanic_test = \
                run_a_table(titanic_table, titanic_indexes,
                    INDICES['survived'], 20, 7, 2)
    titanic_forest_accuray = accuracy(titanic_forest_labels)
    titanic_tree_labels = classify(titanic_train, titanic_test,
                                   INDICES['survived'], titanic_indexes,
                                   titanic_domains)
    titanic_tree_accuracy = accuracy(titanic_tree_labels)

    print('Titanic:')
    print('\tRandom Forest')
    print('\t\tAccuracy = ' + str(titanic_forest_accuray))
    print_confusion_matrix(titanic_forest_labels, 'SURVIVED')
    print('\tDecision Tree')
    print('\t\tAccuracy = ' + str(titanic_tree_accuracy))
    print_confusion_matrix(titanic_tree_labels, 'SURVIVED')

    # auto stuff
    auto_indexes = [INDICES['cylinders'], INDICES['weight'], INDICES['year']]
    auto_domains = table_utils.get_domains(auto_table, auto_indexes)
    auto_forest_labels, auto_train, auto_test = run_a_table(auto_table, auto_indexes,
                    INDICES['mpg'], 20, 7, 2)
    auto_forest_accuray = accuracy(auto_forest_labels)
    auto_tree_labels = classify(auto_train, auto_test,
                                   INDICES['mpg'], auto_indexes,
                                   auto_domains)
    auto_tree_accuracy = accuracy(auto_tree_labels)

    print('Auto:')
    print('\tRandom Forest')
    print('\t\tAccuracy = ' + str(auto_forest_accuray))
    print_confusion_matrix(auto_forest_labels, 'MPG')
    print('\tDecision Tree')
    print('\t\tAccuracy = ' + str(auto_tree_accuracy))
    print_confusion_matrix(auto_tree_labels, 'MPG')


def step_three(auto_table, titanic_table, n, m, f):
    output.printHeader('Step Three: Vary N, M, and F')
    print("N = " + str(n) + " M = " + str(m) + " F = " + str(f))
    # Titanic stuff
    titanic_indexes = [INDICES['age'], INDICES['sex'], INDICES['class']]
    titanic_domains = table_utils.get_domains(titanic_table, titanic_indexes)
    titanic_forest_labels, titanic_train, titanic_test = \
                run_a_table(titanic_table, titanic_indexes,
                    INDICES['survived'], n, m, f)
    titanic_forest_accuray = accuracy(titanic_forest_labels)
    titanic_tree_labels = classify(titanic_train, titanic_test,
                                   INDICES['survived'], titanic_indexes,
                                   titanic_domains)
    titanic_tree_accuracy = accuracy(titanic_tree_labels)

    print('Titanic:')
    print('\tRandom Forest')
    print('\t\tAccuracy = ' + str(titanic_forest_accuray))
    print_confusion_matrix(titanic_forest_labels, 'SURVIVED')
    print('\tDecision Tree')
    print('\t\tAccuracy = ' + str(titanic_tree_accuracy))
    print_confusion_matrix(titanic_tree_labels, 'SURVIVED')

    # auto stuff
    auto_indexes = [INDICES['cylinders'], INDICES['weight'], INDICES['year']]
    auto_domains = table_utils.get_domains(auto_table, auto_indexes)
    auto_forest_labels, auto_train, auto_test = run_a_table(auto_table, auto_indexes,
                    INDICES['mpg'], n, m, f)
    auto_forest_accuray = accuracy(auto_forest_labels)
    auto_tree_labels = classify(auto_train, auto_test,
                                   INDICES['mpg'], auto_indexes,
                                   auto_domains)
    auto_tree_accuracy = accuracy(auto_tree_labels)

    print('Auto:')
    print('\tRandom Forest')
    print('\t\tAccuracy = ' + str(auto_forest_accuray))
    print_confusion_matrix(auto_forest_labels, 'MPG')
    print('\tDecision Tree')
    print('\t\tAccuracy = ' + str(auto_tree_accuracy))
    print_confusion_matrix(auto_tree_labels, 'MPG')

def step_four():
    indexes = util.getValues(WISCONSIN)
    class_index = indexes[-1]
    del indexes[-1]

    table = file_system.loadTable('wisconsin.dat.csv')
    bests = {}
    accs = []

    for n in reversed(range(1, 1001, 250)):
        for m in reversed(range(1, n/2, 300)):
            index_copy = indexes[:]
            for f in reversed(range(1, len(index_copy), 2)):
                labels, train, test = run_a_table(table, index_copy, class_index,
                    n, m, f)
                bests[accuracy(labels)] = (n, m, f)
                accs.append(accuracy(labels))

    # (1001, 301, 7) 0.933039647577

    n, m, f = bests[max(accs)]
    acc = max(accs)

    print 'Best tree has n:', n, 'm:', m, 'f:', f
    print 'and accuracy of', acc

    test, train = holdout(table)
    domains = table_utils.get_domains(table, indexes)
    regular_tree_labels = classify(train, test,
                                   class_index, indexes,
                                   domains)
    reg_accs = accuracy(regular_tree_labels)

    print 'Regular tree accuracy:', reg_accs



def main():
    titanic_table = file_system.loadTable('titanic.txt')
    auto_table = map_auto_cols(file_system.loadTable('auto-data-removedNA.csv'))

    step_two(auto_table, titanic_table)

    #Each of these N, M, and F values were tested 10 times to find the accuracy
    #score for the Random Forest.

    # 1000, 800, 2 = 76-77%
    # 1500, 1000, 2 = 71-77%
    # 800, 200, 3 = 75-77%
    # 400, 300, 3 = 76-77% (most consistently 77%, the winner out of the tested sets)
    # 3000, 2000, 1 = 71-77%
    # 3000, 1500, 2 = 72-77%
    # 1000, 750, 3 = 75-77%
    # 600, 200, 3 = 71-76 #
    step_three(auto_table, titanic_table, 400, 300, 3)

    step_four()
if __name__ == "__main__":
    main()
    
### homework_util

import util
import constants
import table_utils
from tabulate import tabulate

def getDeptEnergyRating(x):
    """ Returns the dept of energy number rating"""

    # Gets the range values for the DOE rankings
    keys     = util.getValues(constants.DOE_RATINGS)

    # Gets the left-end of the range x belongs in
    lowRange = util.getLowRange(keys, x)

    # Flips the dictionary, so we can query by value
    byValue  = util.flipKeyValues(constants.DOE_RATINGS)

    return byValue[lowRange]


def getNHTSASize(x):
    """ Returns the NHTSA Vehicle size """

    # Gets the range values for the DOE rankings
    keys     = util.getValues(constants.NHTSA)

    # Gets the left-end of the range x belongs in
    lowRange = util.getLowRange(keys, x)

    # Flips the dictionary, so we can query by value
    byValue  = util.flipKeyValues(constants.NHTSA)

    return byValue[lowRange]


def convertRowIntoIndexValuePairs(row):
    """ Converts [x, y, z, ...] into [(0, x), (1, y), (2, z)]
    for use in the classifiers in their "where" statements
    """
    return [ (index, value) for index, value in enumerate(row)]


def getNamedTuples(row, *names):
    """ Gets a bunch of tuples by their name
    Ex: getNamedColsFromRow(row, 'mpg', 'origin', 'weight')
        might return [(0, 18.0), (4, 3504), (7, 1)]
    WARNING: These don't necessarily return in any specific order.
    """
    tuples = []
    namesIndexes = [constants.INDICES[name] for name in names]
    for index, value in enumerate(row):
        if index in namesIndexes:
            tuples.append((index, value))
    return tuples

def stratified_cross_fold(table, k, class_index, classify, *opts):
    """
    Uses stratified crossfolding to predict labels
    :param table: Table of data
    :param k: Number of folds
    :param class_index: the class's index
    :param classify: a function to classify on
    :param opts: anything else you'd like to pass into the classify
    :return: labels in format list of tuples [(actual, predicted),...]
    """

    labels = []
    folds = strat_folds(table, class_index, k)
    for index in range(len(folds)):
        test = folds[index]  # creates the test from one fold
        training = []

        # meshes the rest of the folds together to make the training set
        for training_index in range(len(folds)):
            if training_index != index:
                training.extend(folds[training_index])

        labels.extend(classify(training, test, class_index, *opts))
    return labels

def strat_folds(table, by_label_index, k):
    """
    Creates fold where each fold has the same distrubution of class labels as the origonal table

    :param table: table of data
    :param by_label_index: the class label index
    :param k: the number of partitions to create
    :return: a list of tables where each table is a folds, i.e.: [[P1],[P2],..., [Pnum]] where each P is a table
    """
    labels_to_rows = {}

    # spreads the data out into a dictionary where the key is the class label and the data is a table consisting of
    # rows with that class label {class_label:rows_with_class_label
    for row in table:
        label = row[by_label_index]
        try:
            labels_to_rows[label].append(row)
        except KeyError:
            labels_to_rows[label] = [row]

    # creates folds by evenly distributing the rows of each class label to the number of partitions
    folds = {}
    index = 0
    for key, table in labels_to_rows.iteritems():
        for row in table:
            try:
                folds[index].append(row)
            except KeyError:
                folds[index] = [row]
            index += 1
            if index >= k:
                index = 0
    return util.dictionaryToArray(folds)

def print_confusion_matrix(labels, class_label_name):
    """ Prints the confusion matrix of the given labels

    :param labels: A list of tuples of class labels [(actual, predicted),...]
    :param class_label_name: The name of the class label
    """
    class_labels = list(set(table_utils.getCol(labels, 0)))  # all the actual class labels
    the_headers = [class_label_name]
    the_headers.extend(class_labels)
    the_headers.extend(['Total', 'Recognition (%)'])

    # makes an table filled with zeros of #columns = len(the_headers) and #rows = len(class_labels)
    confusion_matrix = [[0] * len(the_headers) for i in range(len(class_labels))]

    # fills out the confusion matrix with the predicted vs. actual
    for a_label_point in labels:
        actual, predicted = a_label_point
        confusion_matrix[class_labels.index(actual)][the_headers.index(predicted)] += 1

    # add the rest of the values to the confusion matrix
    for i in range(len(confusion_matrix)):
        row = confusion_matrix[i]  # current row

        # adding total to the confusion matrix
        total = sum(row)
        row[the_headers.index('Total')] = total  # add the total in for the row

        row[0]= class_labels[i]  # adds the class label for the row to the beginning of row

        # adding recognition to the confusion matrix (% of guesses in row that are correct
        recognition = row[the_headers.index(class_labels[i])] # TP
        recognition /= float(total)
        recognition *= 100
        row[the_headers.index('Recognition (%)')] = recognition

    # prints the table
    print tabulate(confusion_matrix, headers = the_headers, tablefmt="rst")

if __name__ == '__main__':
    row = [18.0,8,307.0,130.0,3504,12.0,70,1,"chevrolet chevelle malibu",2881]
    print getNamedTuples(row, 'mpg', 'origin', 'weight')
    
### constants
#Department of energy ratings
DOE_RATINGS = {
    10: 45,
    9: 37,
    8: 31,
    7: 27,
    6: 24,
    5: 20,
    4: 17,
    3: 15,
    2: 14,
    1: 0
}

ORIGINS = {
    1 : "USA",
    2 : "Europe",
    3 : "Japan"
}

INDICES = {
    'mpg'          : 0,  # auto-data dataset
    'cylinders'    : 1,
    'displacement' : 2,
    'horsepower'   : 3,
    'weight'       : 4,
    'acceleration' : 5,
    'year'         : 6,
    'origin'       : 7,
    'name'         : 8,
    'msrp'         : 9,

    # It's a stupid idea to merge these.
    # He could throw a new dataset at us and then this
    # becomes unusable
    'class'        : 0,  # titanic dataset
    'age'          : 1,
    'sex'          : 2,
    'survived'     : 3
}

WISCONSIN = {
    'clump_thickness'   : 0,
    'cell_size'         : 1,
    'cell_shape'        : 2,
    'marginal_adhesion' : 3,
    'epithelial_size'   : 4,
    'bare_nuclei'       : 5,
    'bland_chromatin'   : 6,
    'normal_nucleoli'   : 7,
    'mitoses'           : 8,
    'tumor'             : 9
}

NHTSA = {
    5 : 3500,
    4 : 3000,
    3 : 2500,
    2 : 2000,
    1 : 0
}

AUTO_HEADERS = [
    'mpg', 'cylinders', 'displacement', 'horsepower', 'weight',
    'acceleration', 'modelyear', 'origin', 'carname', 'msrp'
]

TITANIC_HEADERS = [
    'class', 'age', 'sex', 'survived'
]

### random_forest

import decision_tree
import partition
import heapq
import table_utils
import classifier_util
from homework_util import strat_folds


def run_a_table(table, indexes, class_index, N, M, F):
    """ Takes a table, splits it into a training and test set. Creates a
    random forest for the training set. Then tests the forest off of
    the test set

    :param table: a table of values
    :param indexes: The indexes to partition on
    :param class_index: The index of the label to predict
    :param N: Number of trees to produce
    :param M: Number of the best trees to choose
    :param F: Subset size of random attributes
    :return: Returns a list of tuples. Of the actual, predicted label and
            training and test
             [(actual1,predicted1), (actual2,predicted2), ...], training, test
    """
    domains = table_utils.get_domains(table, indexes)
    folds = strat_folds(table, class_index, 3)
    training = folds[0]
    training.extend(folds[1])
    test = folds[2]
    forest = _random_forest(test, indexes, class_index, domains, N, M, F)

    return [(row[class_index], predict_label(forest, row)) for row in test], \
           training, test


def _random_forest(table, indexes, class_index, att_domains, N, M, F):
    """ Generates a random forest classifier for a given table

    :param table: a table
    :param indexes: a list of indexes to partition on
    :param class_index: the index of the class label to predict
    :param N: Number of trees to produce
    :param M: Number of the best trees to choose
    :param F: Subset size of random attributes
    :return: A list of lists. Trees and thier accuracies
            [(accuracy1, tree1), ... , (accuracyM, treeM)]
    """

    # We store the accuracies and trees in a priority queue
    # lower numbers = higher priority
    priority_queue = [] # see: https://docs.python.org/3/library/heapq.html#basic-examples
    attributes = indexes

    # Uses a training and remainder set from bootsraping to create each tree
    bags = partition.bagging(table, N)
    for bag_set in bags:
        tree = decision_tree.tdidt(bag_set[0], attributes, att_domains, class_index, F)
        acc = _accuracy_for_tree(tree,class_index, bag_set[1])
        heapq.heappush(priority_queue, (acc, tree))
        #push to the priorityQueue

    # Since our priority queue is backwards (and I dunno how to reverse that)
    # we pop off all the ones we don't need. N - M
    for i in range(N - M):
        heapq.heappop(priority_queue)

    # Now our priority queue will be our list that we can return
    return priority_queue


def _accuracy_for_tree(tree, class_index, test_set):
    labels = decision_tree.classify_with_tree(tree, class_index, test_set)
    return classifier_util.accuracy(labels)


def predict_label(forest, instance):
    """ predicts the label of an instance given a forest using weighted
        voting with accuracies

    :param forest: a list of lists in te form returned by random_forest()
    :param instance: an row to have a class label predicted
    :return: a class label
    """
    labels = {}
    for acc_and_tree in forest:
        prediction = decision_tree.get_label(acc_and_tree[1], instance)
        # totals the accuracy predicted for each label
        try:
            labels[prediction] += acc_and_tree[0]
        except KeyError:
            labels[prediction] = acc_and_tree[0]

    # gets the label with the highest predicted value
    highest_value = 0
    highest_label = 0
    for current_label, value in labels.items():
        if value > highest_value:
            highest_label = current_label

    return highest_label

### classifier_util

import util
import numpy
import table_utils

def _split(labels):
    """ Divides up the labels by their actual label
    returns a dictionary of labels
    """
    groups = {}

    for label in labels:
        actual, predicted = label

        try:
            groups[actual].append((actual, predicted))
        except KeyError:
            groups[actual] = [(actual, predicted)]

    return groups

def accuracy(labels):
    """
    Returns multiclass accuracy

    :param labels: list of tuples with [(actual, predicted), ...]
    :return: accuracy
    """
    correct = 0
    all_labels = list(set([labels[i][0] for i in range(len(labels))]))  # set of all the possible labels
    accuracies = [0] * len(all_labels)  # accuracies of each label

    for i in range(len(accuracies)):
        current_label = all_labels[i]
        for label in labels:
            actual, predicted = label
            if actual == current_label and predicted == current_label:
                accuracies[i] += 1  # true positives
            elif actual != current_label and predicted != current_label:
                accuracies[i] += 1  # true negatives

    length = len(labels)
    # Does the mean of all the (TP + TN) / (All labels)
    return numpy.mean([float(an_accuracy) / float(length) for an_accuracy in accuracies])


def _findFalses(splitLabels, forLabel):
    """ Takes an array of labels, counts the number of FP and FN """

    falses = 0
    for row in splitLabels:
        for actual, predicted in row:

            # If either the predicted or actual is the label we care about
            if actual == forLabel:
                if actual != predicted: #if label is false
                    falses += 1
            elif predicted == forLabel:
                if actual != predicted:
                    falses += 1
    return falses


if __name__ == '__main__':
    labels = [(1, 1), (1, 4), (1, 2), (2, 2), (2, 3), (3, 3)]
print _split(labels)
