#####
# hw3.py
#####

import file_system
import constants
import math_utils
import numpy
import hw3_utils as hw3
import dataOperations as dataOp
import classifiers
import util
import csv
import random
import math
import tabulate

# Step 1
#
# Build a linear regression from
# weight to MPG points,
# then use that to predict the mpg
def linearRegressionClassifier(table):

    # Create a list of 2d points with weight and mpg
    points = hw3.getPoints(table, 'weight', 'mpg')

    # Make the linear regression funtion to get MPG
    getMPG = math_utils.linear_regression(points)

    # Get 5 random instances
    instances = hw3.getInstances(table, 5)

    for instance in instances:
        hw3.printInstance(instance)
        classValue  = getMPG(hw3.get(instance, 'weight'))
        classValue  = dataOp.getDeptEnergyRating(classValue)
        actualValue = hw3.get(instance, 'mpg')
        actualValue = dataOp.getDeptEnergyRating(actualValue)
        hw3.printClassActual(classValue, actualValue)

# Step 2
# Builds a K-Nearest-Neighbor
def kNearestNeighborClassifier(table):
    normalizedTable = util.normalizeTable(table, constants.INDICES['mpg'])
    trainingSet, testSet = hw3.splitInstances(normalizedTable, len(normalizedTable)-5)

    k = 5

    for index, instance in enumerate(testSet):
        hw3.printInstance(table[index])
        classValue  = classifiers.k_nn(trainingSet,
            instance, len(instance), k, constants.INDICES['mpg'])
        classValue  = dataOp.getDeptEnergyRating(classValue)
        actualValue = hw3.get(instance, 'mpg')
        actualValue = dataOp.getDeptEnergyRating(actualValue)
        hw3.printClassActual(classValue, actualValue)

# Step 3
# Builds predictive accuracy
def computePredictiveAccuracy(table):
    normalizedTable = util.normalizeTable(table, constants.INDICES['mpg'])
    knn_acc, knn_error = hw3.randomSubsampleKnn(normalizedTable, 10)
    lin_acc, lin_error = hw3.randomSubsampleLinear(normalizedTable, 10)
    knn_c_acc, knn_c_error = hw3.crossFoldKnn(normalizedTable)
    lin_c_acc, lin_c_error = hw3.crossFoldLinear(normalizedTable)

    space = '   '
    knn = 'k Nearest Neighbors: accuracy ='
    linreg = 'Linear Regression: accuracy ='
    print "Random Subsample"
    print space, linreg, lin_acc, 'error rate=', lin_error
    print space, knn, knn_acc, 'error rate =', knn_error
    print "Stratified 10-Fold Cross Validation"
    print space, linreg, lin_c_acc, 'error rate=', lin_c_error
    print space, knn, knn_c_acc, 'error rate=', knn_c_error

def confusionMatrix(table):
    headers = ['MPG', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'Total', 'Recognition (%)']
    left = list(xrange(1, 10, 1))

    normalizedTable = util.normalizeTable(table, constants.INDICES['mpg'])
    confusionTable = hw3.confusionMatrixTable(normalizedTable)

    print tabulate.tabulate(confusionTable, headers, tablefmt='rst')


def main():
    table = file_system.loadTable('auto-data-removedNA.csv')

    # Step 1
    hw3.printHeader('STEP 1: Linear Regression MPG Classifer')
    linearRegressionClassifier(table)

    # Step 2
    hw3.printHeader('STEP 2: k=5 Nearest Neighbor MPG Classifier')
    kNearestNeighborClassifier(table)

    # Step 3
    hw3.printHeader('STEP 3: Predictive Accuracy')
    computePredictiveAccuracy(table)

    hw3.printHeader('STEP 4: Confusion Matrix')
    confusionMatrix(table)


if __name__ == "__main__":
    main()







#####
# hw3_utils.py
#####


import file_system
import constants
import math_utils
import numpy
from operator import itemgetter
import random
import util
from random import randint
import classifiers
import dataOperations as dataOp
from collections import Counter
import tabulate

# Gets by name instead of by index
def get(row, name):
    index = constants.INDICES[name]
    return row[index]

# Gets a 2d array of points, from the index
# names.
#
# Example:
#   getPoints(table, 'name', 'msrp')
# would give
# [['chevrolet monte carlo', 3123], ['nameofcar', 2312]]
#
def getPoints(table, *names):
    points = []
    for row in table:
        point = []
        for name in names:
            point.append(get(row, name))
        points.append(point)
    return points

# Gets a number of random instances from the table
def getInstances(table, num):
    return random.sample(table, num)

# Returns training, then test sets
def splitInstances(table, numTraining):
    indices = range(len(table))
    trainingIndices = random.sample(indices, numTraining)
    testIndices = [item for item in indices if item not in trainingIndices]
    return ([table[i] for i in trainingIndices],
        [table[i] for i in testIndices])

# Returns test then training sets
def holdout_partition(table):
    randomized = table[:]
    n = len(table)
    for i in range(n):
        j = randint(0, n-1)
        randomized[i], randomized[j] = randomized[j], randomized[i]
    n0 = (n * 2)/3
    return randomized[0:n0], randomized[n0:]

# Creates stratified cross validation folds where
# the fold's data has the same class label
def stratFolds(table, byLabelIndex, numOfPartitions):
    randomized = table[:]
    partitions = {}

    for row in table:
        label = row[byLabelIndex]
        try:
            partitions[label].append(row)
        except KeyError:
            partitions[label] = [row]

    folds = {}
    index = 0
    for key, table in partitions.iteritems():
        for row in table:
            try:
                folds[index].append(row)
            except KeyError:
                folds[index] = [row]
            index += 1
            if index > numOfPartitions:
                index = 0
    return util.dictionaryToArray(folds)

# Gets a number of random instances from the table
def getInstances(table, num):
    numInstances = len(table)
    choices = numpy.random.choice(numInstances, num)
    return [table[i] for i in choices]

# Splits the dataset so 2/3 of the data is in the
# training set and 1/3 of athe data is in the
# test set.
def splitDataset(filename, split, trainingSet=[] , testSet=[]):
	with open(filename, 'rb') as csvfile:
	    lines = csv.reader(csvfile)
	    dataset = list(lines)
	    for x in range(len(dataset)-1):
	        for y in range(4):
	            dataset[x][y] = float(dataset[x][y])
	        if random.random() < split:
	            trainingSet.append(dataset[x])
	        else:
	            testSet.append(dataset[x])

# Finds the euclidean distance
def euclideanDistance(instance1, instance2, length):
	distance = 0
	for x in range(length):
		distance += pow((instance1[x] - instance2[x]), 2)
	return math.sqrt(distance)

# Get the array as a comma seperated string, then kill
# the last comma
def getInstanceString(instance):
    return ''.join(str(i)+', ' for i in instance).strip()[:-1]

def printInstance(instance):
    print 'instance:', getInstanceString(instance)

def printClassActual(classValue, actualValue):
    print 'class:', classValue, 'actual:', actualValue

def printHeader(text):
    print '==========================================='
    print text
    print '==========================================='

def accuracy(classValues, actualValues):
    total = len(classValues)
    correct = 0
    for i in range(len(classValues)):
        if classValues[i] == actualValues[i]:
            correct += 1
    return correct*1.0/total*1.0

def knn(trainingSet, testSet, k):
    classValues = []
    actualValues = []
    for index, instance in enumerate(testSet):
        classValue  = classifiers.k_nn(trainingSet,
            instance, len(instance), k, constants.INDICES['mpg'])
        classValues.append(dataOp.getDeptEnergyRating(classValue))
        actualValue = get(instance, 'mpg')
        actualValues.append(dataOp.getDeptEnergyRating(actualValue))

    acc = accuracy(classValues, actualValues)
    error = 1 - acc

    return acc, error

def linearRegression(trainingSet, testSet):

    # Make the linear regression funtion to get MPG
    getMPG = math_utils.linear_regression(trainingSet)

    classValues = []
    actualValues = []
    for instance in testSet:
        classValue  = getMPG(get(instance, 'weight'))
        classValue  = dataOp.getDeptEnergyRating(classValue)
        classValues.append(classValue)
        actualValue = get(instance, 'mpg')
        actualValue = dataOp.getDeptEnergyRating(actualValue)
        actualValues.append(actualValue)

    acc = accuracy(classValues, actualValues)
    error = 1 - acc
    return acc, error

def randomSubsampleLinear(table, k):
    accs = []
    errors = []
    for i in range(k):
        trainingSet, testSet = holdout_partition(table)
        accuracy, error = linearRegression(trainingSet, testSet)
        accs.append(accuracy)
        errors.append(error)

    return numpy.mean(accs), numpy.mean(errors)

def randomSubsampleKnn(table, k):
    accs = []
    errors = []
    for i in range(k):
        trainingSet, testSet = holdout_partition(table)
        accuracy, error = knn(trainingSet, testSet, 4)
        accs.append(accuracy)
        errors.append(error)

    return numpy.mean(accs), numpy.mean(errors)

def crossFoldLinear(table):
    folds = stratFolds(table, constants.INDICES['mpg'], 10)

    accs = []
    errors = []
    for fold in folds:
        trainingSet, testSet = holdout_partition(fold)
        accuracy, error = linearRegression(trainingSet, testSet)
        accs.append(accuracy)
        errors.append(error)

    return numpy.mean(accs), numpy.mean(errors)

def crossFoldKnn(table):
    folds = stratFolds(table, constants.INDICES['mpg'], 10)

    accs = []
    errors = []
    for fold in folds:
        trainingSet, testSet = holdout_partition(fold)
        accuracy, error = knn(trainingSet, testSet, 4)
        accs.append(accuracy)
        errors.append(error)

    return numpy.mean(accs), numpy.mean(errors)

def knnForActuals(trainingSet, testSet, k):
    points = []
    for index, instance in enumerate(testSet):
        classValue  = classifiers.k_nn(trainingSet,
            instance, len(instance), k, constants.INDICES['mpg'])
        classes = dataOp.getDeptEnergyRating(classValue)

        actualValue = get(instance, 'mpg')
        actual = dataOp.getDeptEnergyRating(actualValue)

        points.append([actual, classes])

    return points

def confusionMatrixTable(table):
    folds = stratFolds(table, constants.INDICES['mpg'], 10)

    pointsList = []

    for fold in folds:
        trainingSet, testSet = holdout_partition(fold)
        points = knnForActuals(trainingSet, testSet, 4)
        pointsList.extend(points)

    # Add core data
    graphData = [[0 for i in xrange(11)] for i in xrange(10)]
    for point in pointsList:
        graphData[point[1]-1][point[0]] += 1

    # Add Total
    for row in graphData:
        row.append(sum(row))

    # Add Recognition %
    for index, row in enumerate(graphData):
        total = row[-1]
        guessedRight = row[index]

        if total != 0:
            row.append(guessedRight * 1.0 / total)
        else:
            row.append(0)

    # Add left side
    for index, row in enumerate(graphData):
        row[0] = index + 1

    return graphData


if __name__ == "__main__":
    table = [
        [1, 2, 'hi'],
        [0, 2, 'hi'],
        [1, 1, 'hi'],
        [1, 2, 'by'],
        [1, 2, 'by'],
        [1, 3, 'by'],
        [13, 2, 'by'],
        [1, 2, 'hi'],
    ]



#####
# classifiers.py
#####

import math
import util
import collections

# Gets the smallest element in the array of tuples
# and returns the index and value
def _smallest(rowTuples):
    smallest = None
    smallestIndex = None
    for index, tup in enumerate(rowTuples):
        if smallest == None or tup[0] < smallest:
            smallest = tup[0]
            smallestIndex = index
    return smallestIndex, smallest

# Gets the top k rows
def _get_top_k(rowTuples, k):
    tops = []

    for tup in rowTuples:
        # If we haven't filled tops yet, append it
        if len(tops) < k:
            tops.append(tup)
            continue

        smallestIndex, smallest = _smallest(tops)
        if (tup[0] > smallest):
            tops[smallestIndex] = tup

    return tops

def _findWhere(combinedRows, test):
    for index, combined in enumerate(combinedRows):
        distance, row = combined
        if distance == test:
            return combined
    return None

def _roundDistances(combinedRows):
    newRows = []
    for combination in combinedRows:
        row = (round(combination[0], 6), combination[1])
        newRows.append(row)
    return newRows


# Picks the most common class label
def _select_class_label(commonRows, labelIndex):
    rounded = _roundDistances(commonRows)
    distances = [combined[0] for combined in rounded]
    mostCommonCombined = collections.Counter(distances).most_common()
    mostCommon = _findWhere(rounded, mostCommonCombined[0][0])
    distance, row = mostCommon
    return row[labelIndex]


# Calculate distances
# If the two items are catigorical, it will
# say a distance of 0 if they are the same,
# and a distance of 1 if they are different
def _distance(row, instance, size, labelIndex):
    parts = []
    for i in range(size):
        # Don't include label in sum
        if i == labelIndex:
            continue

        ai = row[i]
        bi = instance[i]

        if isinstance(ai, basestring) or isinstance(bi, basestring):
            part = 0 if ai == bi else 1
            parts.append(pow(part, 2)) # raised to 2 for s's & g's
            continue
        else:
            parts.append(pow((ai - bi), 2))
            continue
    return sum(parts)


def k_nn(training_set, instance, n, k, labelIndex):
    row_distances = []
    for row in training_set:
        d = _distance(row, instance, n, labelIndex)
        row_distances.append((d, row))
    top_k_rows = _get_top_k(row_distances, k)
    label = _select_class_label(top_k_rows, labelIndex)
    return label

if __name__ == "__main__":
    distances = [(2, []), (7, []), (6, []), (3, [])]
    first = [2, 'hi']
    second = [3.5, 'hi']
    print 'dist', _distance(first, second, 2)


#####
# math_utils.py
#####

import util
import numpy
import math

def slope(points, x_bar, y_bar):
    top_values = []
    bot_values = []
    for point in points:
        x = point[0]
        y = point[1]
        top_values.append((x - x_bar)*(y - y_bar))
        bot_values.append((x - x_bar)*(x - x_bar))

    return sum(top_values)/sum(bot_values)

def correlationCoeff(points):
    xs = util.getCol(points, 0)
    ys = util.getCol(points, 1)
    x_bar = numpy.mean(xs)
    y_bar = numpy.mean(ys)

    x_delta = []
    y_delta = []
    x_bot_delta = []
    y_bot_delta = []
    for point in points:
        x = point[0]
        y = point[1]

        topX = (x - x_bar)
        topY = (y - y_bar)
        x_delta.append(topX)
        y_delta.append(topY)
        x_bot_delta.append(topX*topX)
        y_bot_delta.append(topY*topY)

    top = sum(x_delta)*sum(y_delta)
    bot = math.sqrt(sum(x_bot_delta)*sum(y_bot_delta))
    return top/bot

def covariance(points):
    xs = util.getCol(points, 0)
    ys = util.getCol(points, 1)
    x_bar = numpy.mean(xs)
    y_bar = numpy.mean(ys)

    def topPart(point):
        return math.pow((point[0] - x_bar),2) * math.pow((point[1] - y_bar),2)

    top = [ topPart(point) for point in points ]
    return sum(top)/(len(points)-1)

def linear_regression(points):
    xs = util.getCol(points, 0)
    ys = util.getCol(points, 1)
    x_bar = numpy.mean(xs)
    y_bar = numpy.mean(ys)

    m = slope(points, x_bar, y_bar)
    b = y_bar - m*x_bar

    def line(x):
        return m*x + b
    return line

def graphablePoints(points):
    xs = util.getCol(points, 0)
    max_x = max(xs)
    min_x = min(xs)
    getY = linear_regression(points)

    return [[min_x, getY(min_x)], [max_x, getY(max_x)]]

if __name__ == '__main__':
    print covariance([[0, 1], [1.5, 2], [2, 4], [8, 5]])
